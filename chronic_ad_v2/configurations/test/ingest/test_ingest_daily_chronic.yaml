# Timezone (Used for time related conversion)
tz: Asia/Singapore
analysis_type: daily_chronic
datetime_standard_format: '%Y-%m-%d %H:%M:%S'
log_relative_dir: "../logs"
SPARK:
  spark.executor.memory: 4g
  spark.executor.instances: '50'
  spark.driver.memory: 6g
  spark.yarn.queue: eng_f10w-01
  spark.yarn.principal: hdfsf10n@NA.MICRON.COM
  spark.yarn.keytab: /home/hdfsf10n/.keytab/hdfsf10n.keytab
  spark.app.name: SPC_CHRONIC_OOC_AD_OOC_Query_pengtan_test
  spark.master: yarn

MSSQL:
  # Tracking table for AD
  tracking_table_name: ad_v2t_tracking
  # Staging table for AD Tracking
  tracking_staging_table_name: ad_v2t_as_staging_tracking
  # Samples table for AD
  sample_table_name: ad_v2t_samples
  # Staging table for AD Samples
  sample_staging_table_name: ad_v2t_as_staging_samples
  spc_chronic_table_name: ad_v2_spc_chronic
  # channel folder table
  ch_folder_table_name: ad_v2_ch_folder
  # database used
  database: F10DS_SOLUTION
  # Hostname
  server: FSWSQL165\FSMSSPROD165
  # user name for mssql (need to write permission)
  user: mssqlf10n
  password: TWljcm9uMTIz
  port: 1433

FILTER:
  query_ooc_only_flag: no
  # This is designed to handle min 1 OOC in last 24 hours
  ooc_sample_latest_check_in_hours: 720
  latest_ooc_min_count: 0
  # This is designed to handle min 2 OOCS in 3 days
  #  ooc_sample_num_check_in_hours: 720
  #  min_ooc_count: 2
  # Total query period, eg, 30 days
  space_sample_period_in_hours: 720
  # Used if multiple sessions to go in one run
  query_interval_in_seconds: 86400
  # Cutoff hour, for example, if 21 is specified, 2018/12/16 21:00:00 to 2018/12/17 21:00:00 is used
  # instead of 2018/12/16 00:00:00 to 2018/12/17 00:00:00
  # put -1 if not enabled
  cutoff_hour: 5
  # violation type list
  #
  vio_type_list_csv:  3,4,5,6,11,12,17,18
  buffer_seconds: 10800
  area: []

CHRONIC:
  chronic_chart_csv_path: "/etlstage/Chronic_Dashboard/final_data_new.csv"
  datetime_origin_format: "%m/%d/%Y"
  back_check_days: 3



